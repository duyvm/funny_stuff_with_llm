{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMTXPD4CdoR2QHrgm0fDNA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duyvm/funny_stuff_with_llm/blob/main/learning-rag/Langchain_multiqueryretriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ym6NvY0p5ek"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet --upgrade langchain-chroma langchain[openai] langchain langchain-community langgraph langchain-core langchain-text-splitters> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = f\"langchain-learning-rag\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get('LANGSMITH_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "xmtEhKrEACgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "Guide: [How to use MultiQueryRetriever](https://python.langchain.com/docs/how_to/MultiQueryRetriever/)\n",
        "\n",
        "- Retrieval may produces different results if there is subtle changes in query wordings or embeddings do not capture semantic meaning of data.\n",
        "\n",
        "- Using `MultiQueryRetriever` automates the process of prompt tuning by leveraging llm to generate multiple queries from different perspectives for a user input.\n",
        "\n",
        "**Detail methods**\n",
        "\n",
        "1. For given query, use llm to generate multiple queries\n",
        "\n",
        "2. Retrieve documents using those queries\n",
        "\n",
        "3. Union unique documents from retrieval results and pass them as context for answer synthesis"
      ],
      "metadata": {
        "id": "j60Ksa78BWHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data\n",
        "\n",
        "Using the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng"
      ],
      "metadata": {
        "id": "F7MiQWGzCE4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "data = loader.load()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "docs = splitter.split_documents(data)\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectordb = Chroma.from_documents(docs, embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQp8zoPMwQkx",
        "outputId": "f694a1c7-9072-4924-d276-6947858f7781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "question = \"What are the approaches to Task Decomposition?\"\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
        "    retriever=vectordb.as_retriever(), llm=llm\n",
        ")"
      ],
      "metadata": {
        "id": "yw6PKWu9DS3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "ExY7ibze_4qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_docs = retriever_from_llm.invoke(question)\n",
        "len(unique_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb1XO8OeuBes",
        "outputId": "c6e3bc93-1b04-4876-e811-050982812546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. How can Task Decomposition be achieved through different methods?', '2. What strategies can be used for breaking down tasks in Task Decomposition?', '3. What are the various techniques for approaching Task Decomposition effectively?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Default `MultiQueryRetriver` prompt: [MultiQueryRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html)\n",
        "\n",
        "- Use your own prompt:\n",
        "\n",
        "  1. Make a `PromptTemplate`\n",
        "\n",
        "  2. Implement an output parser"
      ],
      "metadata": {
        "id": "SHWfHDp5CjyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from langchain_core.output_parsers import BaseOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class LineListOutputParse(BaseOutputParser[List[str]]):\n",
        "    \"\"\"Output parser for list of lines.\"\"\"\n",
        "    def parse(self, text: str) -> List[str]:\n",
        "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
        "        lines = text.strip().split(\"\\n\")\n",
        "        return list(filter(None, lines))\n",
        "\n",
        "ouput_parser = LineListOutputParse()\n",
        "\n",
        "QUERY_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "    different versions of the given user question to retrieve relevant documents from a vector\n",
        "    database. By generating multiple perspectives on the user question, your goal is to help\n",
        "    the user overcome some of the limitations of the distance-based similarity search.\n",
        "    Provide these alternative questions seperated by newlines.\n",
        "    Original question: {question}\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "chain = QUERY_PROMPT | llm | ouput_parser"
      ],
      "metadata": {
        "id": "HhVw7jGiC8b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = MultiQueryRetriever(\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    llm_chain=chain,\n",
        "    parser_key=\"lines\",\n",
        ")\n",
        "\n",
        "unique_docs = retriever.invoke(\"What does the course say about regression?\")\n",
        "len(unique_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YtpnN3oLoQN",
        "outputId": "0fdcc49b-8f46-41f6-c447-7736ab49154a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide information on regression as discussed in the course?', '2. What topics related to regression are covered in the course material?', '3. How is regression explained in the course content?', '4. What insights does the course offer on regression?', '5. What are the key points regarding regression in the course curriculum?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cjiUD1FYMOia"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}