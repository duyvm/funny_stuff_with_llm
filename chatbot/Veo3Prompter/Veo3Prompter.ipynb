{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "13qg7GTAunnQESI-_TpSAhPtAFGESEjT5",
      "authorship_tag": "ABX9TyPeh+lR6hSDW0Hv2E+U/uFq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duyvm/funny_stuff_with_llm/blob/main/ChatBot/Veo3Prompter/Veo3Prompter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install required packages\n",
        "\n",
        "- langchain,langgraph, openai for building llm application\n",
        "- langsmith (optional) is used for monitor your AI application (access the dashboard at your project https://smith.langchain.com)\n",
        "- gradio for UI interaction\n",
        "- openai for working with openai model"
      ],
      "metadata": {
        "id": "hrhG3bXbK8pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-core langgraph>0.2.27 openai gradio\n",
        "!pip install -qU \"langchain[openai]\""
      ],
      "metadata": {
        "id": "fH-JnJ4rK_ms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5ba735d-38ac-4c5b-a3ca-62bccf315507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/65.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up environment variables\n",
        "\n",
        "- Setup environment variables for langsmith (optional) and openai\n",
        "- Load secrets from Google Colab secret."
      ],
      "metadata": {
        "id": "Zidn2Y6rLE11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# basic Python/Google Colab libraries\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "from datetime import datetime\n",
        "from google.colab import userdata\n",
        "from typing import Sequence\n",
        "from typing_extensions import Annotated, TypedDict\n",
        "\n",
        "### LLM libraries\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage, trim_messages\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "### UI libraries\n",
        "import gradio as gr\n",
        "import asyncio\n",
        "\n",
        "date_str = datetime.now().strftime(\"%Y%m%d\")\n",
        "\n",
        "# Langsmiths\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = f\"veo3-promter-{date_str}\"\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass('Please enter your langsmith key: ')\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get('LANGSMITH_API_KEY')\n",
        "\n",
        "# OpenAI\n",
        "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass('Please enter your openai key: ')\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "50a35JUZsd2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions\n",
        "\n",
        "- Helper functions for save/load files"
      ],
      "metadata": {
        "id": "J3-adPI0sCde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_FILE_PATH = \"/content/drive/MyDrive/ColabNotebooks/Chatbot/VEO3Prompter\"\n",
        "\n",
        "def save_history_to_file(filename: str, messages: Sequence[BaseMessage]) -> None:\n",
        "    \"\"\"\n",
        "    Write messages into json file\n",
        "    \"\"\"\n",
        "    filepath = Path(DEFAULT_FILE_PATH+filename)\n",
        "    data = [{\"type\": msg.__class__.__name__, \"content\": msg.content} for msg in messages]\n",
        "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "\n",
        "def load_history_from_file(filename: str) -> Sequence[BaseMessage]:\n",
        "    \"\"\"\n",
        "    Read messages from json file\n",
        "    \"\"\"\n",
        "    filepath = Path(DEFAULT_FILE_PATH+filename)\n",
        "    if not filepath.exists():\n",
        "        return []\n",
        "\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw_data = json.load(f)\n",
        "\n",
        "    messages = []\n",
        "    for item in raw_data:\n",
        "        if item[\"type\"] == \"HumanMessage\":\n",
        "            messages.append(HumanMessage(content=item[\"content\"]))\n",
        "        elif item[\"type\"] == \"AIMessage\":\n",
        "            messages.append(AIMessage(content=item[\"content\"]))\n",
        "    return messages\n",
        "\n",
        "def to_gradio_pair_msgs(messages: Sequence[BaseMessage]):\n",
        "    \"\"\"\n",
        "    Convert messages to gradio list of pair messages\n",
        "    \"\"\"\n",
        "    return [(messages[i].content, messages[i + 1].content) if i + 1 < len(messages) else (messages[i].content, \"\") for i in range(0, len(messages), 2)]"
      ],
      "metadata": {
        "id": "fX2mDdSssWBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Chatbot - powered by LLM\n",
        "\n",
        "Basic functionalities\n",
        "\n",
        "| Feature                    | Status |\n",
        "| -------------------------- | ------ |\n",
        "| file-based store chat history    | ✅      |\n",
        "| Load history on refresh    | ✅      |\n",
        "| Save history after reply   | ✅      |\n",
        "| Interactive UI | ✅      |\n",
        "| Help user on creating prompt for video generation by Veo3 | ✅      |\n",
        "\n",
        "Further improvements\n",
        "- Agentic approach\n",
        "- Full-fledged web app using library like Streamlit (FE), FastAPI or Flask (API server), DB for storing messages\n",
        "- Implement user sessions management\n",
        "- Implement token management"
      ],
      "metadata": {
        "id": "VYv6zUSwQ3-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_ROLE = \"\"\"\n",
        "Act as a visionary film director who collaborates with the user to transform raw ideas into vivid, story-driven cinematic sequences using Google’s Veo 3 video generation platform.\n",
        "\n",
        "You do not act like a prompt engineer. You direct scenes — with the perspective, creativity, and responsibility of a real filmmaker.\n",
        "\n",
        "Your job is to guide the user through a structured cinematic creative process, helping them define story moments, characters, mood, visuals, and camera direction. Then, you translate these into detailed prompts formatted for Veo 3.\n",
        "\n",
        "Your responses must be in {language} and follow the creative workflow below.\n",
        "\n",
        "---\n",
        "\n",
        "CREATIVE WORKFLOW\n",
        "\n",
        "Step 1: Understand the Vision\n",
        "Begin by asking the user:\n",
        "1. What is the overall idea? (the story, theme, or concept)\n",
        "2. What is the purpose of the video? (e.g. storytelling, advertising, education, mood piece)\n",
        "3. What is the total video length?\n",
        "\n",
        "Note: Veo 3 supports only 8 seconds per clip. Longer stories must be split into multiple scenes.\n",
        "\n",
        "---\n",
        "\n",
        "Step 2: Gather Cinematic Details\n",
        "Ask the user to describe:\n",
        "\n",
        "1. Scene Action\n",
        "   - What happens in the scene? Who does what?\n",
        "   - Use action verbs and clear phrasing.\n",
        "\n",
        "2. Setting / Environment\n",
        "   - Where is the scene set? Indoors or outdoors? Time of day? Weather?\n",
        "   - Add physical details: architecture, nature, motion in the background.\n",
        "\n",
        "3. Characters\n",
        "   - Who is in the scene? Include:\n",
        "     - Number, age, gender, clothing, physical traits (hair, eyes, face...)\n",
        "     - Emotional state and personality\n",
        "     - Name and notable features (glasses, jewelry, makeup, tattoos, etc.)\n",
        "\n",
        "4. Visual Style\n",
        "   - What does the scene look like? Choose from: cinematic, noir, anime, cyberpunk, historical, Pixar-style, etc.\n",
        "   - You may reference directors (e.g. “Wes Anderson-style”) or films.\n",
        "\n",
        "5. Camera Direction\n",
        "   - How is the scene shot? Use cinematic terms:\n",
        "     - Wide shot, close-up, drone shot, tracking, push-in, POV, etc.\n",
        "   - Describe movement, angle, and pace.\n",
        "\n",
        "6. Lighting\n",
        "   - Define lighting style: warm sunset, flickering candlelight, harsh daylight, neon-lit, etc.\n",
        "\n",
        "7. Sound Design\n",
        "   - Describe what’s heard: music, ambient noise, sound effects, dialogue.\n",
        "   - Mention volume, clarity, and atmosphere.\n",
        "\n",
        "8. Tone / Emotion\n",
        "   - What’s the mood? Peaceful, eerie, tense, magical, nostalgic, etc.\n",
        "\n",
        "9. Dialogue (optional)\n",
        "   - Provide exact lines of dialogue and specify:\n",
        "     - Who speaks (by character name)\n",
        "     - What is said, in what language\n",
        "     - At what timestamp (e.g., “At 2s–3s, Mary whispers: ‘Let’s go.’”)\n",
        "\n",
        "---\n",
        "\n",
        "Step 3: Break the Story into Scenes\n",
        "If the video is longer than 8 seconds:\n",
        "\n",
        "- Divide it into short scenes like a storyboard.\n",
        "- Each scene must include:\n",
        "  - Specific action or moment\n",
        "  - Defined setting and lighting\n",
        "  - Consistent tone and characters\n",
        "\n",
        "⚠️ Important: Ensure continuity. The final frame of one scene should become the first frame of the next, when appropriate.\n",
        "\n",
        "Confirm the scene breakdown with the user before moving forward.\n",
        "\n",
        "---\n",
        "\n",
        "Step 4: Generate Keyframe Prompts (Optional but Recommended)\n",
        "For each scene:\n",
        "- Create a **keyframe prompt** describing the opening moment as a static image.\n",
        "- These are used with tools like Midjourney, Sora, Gemini, or Veo’s Image-to-Video feature.\n",
        "- Keyframes lock in consistent visual style, characters, and mood before animation.\n",
        "\n",
        "**Use detailed descriptions of character, outfit, lighting, pose, and camera angle.**\n",
        "\n",
        "Always include negative prompts for things to avoid.\n",
        "\n",
        "---\n",
        "\n",
        "Step 5: Direct Each Scene with a Cinematic Prompt\n",
        "Once scenes are confirmed, generate one 500 characters cinematic prompt per scene.\n",
        "The prompt must include as detail as possible.\n",
        "\n",
        "🧠 Think like a director. Don't just describe objects — describe moments.\n",
        "\n",
        "🎥 Always include:\n",
        "- Full character description for consistency (appearance, outfit, mood)\n",
        "- Clear setting and time\n",
        "- Camera technique and angle\n",
        "- Lighting and sound\n",
        "- Emotional tone\n",
        "- End each with “No subtitles.”\n",
        "\n",
        "🧱 Use this format:\n",
        "[Subject + Action] + [Time + Place] + [Camera Movement + Visual Style] + [Lighting + Sound Design] + [Tone or Emotion]\n",
        "\n",
        "⚠️ Important: The prompt must be in English and ending with \"No subtitles.\"\n",
        "\n",
        "---\n",
        "\n",
        "🎨 Keyframe Example Prompt (For Image-to-Video)\n",
        "\n",
        "Character:\n",
        "- A young Vietnamese woman, 22 years old\n",
        "- Oval-shaped face, smooth light-medium skin, long black hair, side part\n",
        "- Almond-shaped dark brown eyes, full lips with a soft pink hue\n",
        "- Wearing a white short-sleeve linen blouse and light-blue jeans\n",
        "- Minimalist silver necklace\n",
        "- Calm, friendly expression, three-quarter view, soft daylight\n",
        "- Background: Blurred neutral café interior\n",
        "- Art Style: Photorealistic, high detail, sharp focus\n",
        "- Negative Prompts: No hats, heavy makeup, glasses, distracting patterns\n",
        "\n",
        "---\n",
        "\n",
        "🎬 Director's Best Practices\n",
        "\n",
        "- Describe **moments**, not just things.\n",
        "- Use **light and sound** to shape the emotional tone.\n",
        "- Always embed **character consistency** across all scenes.\n",
        "- Ask the user:\n",
        "  “Do you want the camera to follow the action, or observe it from a distance?”\n",
        "\n",
        "---\n",
        "\n",
        "🚫 RESTRICTIONS\n",
        "\n",
        "Only assist with cinematic storytelling and prompt generation for Veo 3.\n",
        "\n",
        "If the user asks about anything unrelated, respond:\n",
        "**\"I can’t help with that. Would you like help directing a cinematic scene instead?\"**\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sejAGM1AkZKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            SYSTEM_ROLE,\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "class State(TypedDict):\n",
        "            messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "            language: str\n",
        "\n",
        "class Veo3Prompter():\n",
        "\n",
        "    def __init__(self, thread_id: str, model_name: str = \"gpt-4o-mini\", model_provider: str = \"openai\"):\n",
        "        self.__init_model__(model_name, model_provider)\n",
        "        self.__init_app__()\n",
        "        self.history_file = f\"{thread_id}.json\"\n",
        "        self.messages = load_history_from_file(self.history_file)\n",
        "\n",
        "    def __init_app__(self):\n",
        "        workflow = StateGraph(state_schema=State)\n",
        "\n",
        "        # Define the (single) node in the graph\n",
        "        workflow.add_edge(START, \"model\")\n",
        "        workflow.add_node(\"model\", self.call_model)\n",
        "\n",
        "        memory = MemorySaver()\n",
        "        self.app = workflow.compile(checkpointer=memory)\n",
        "\n",
        "    def __init_model__(self, model_name: str = \"gpt-4o-mini\", model_provider: str = \"openai\"):\n",
        "        if model_provider.lower() == \"openai\":\n",
        "            self.model = init_chat_model(model_name, model_provider=model_provider)\n",
        "        else:\n",
        "            raise Exception(\"Unsupported model provider\")\n",
        "\n",
        "    async def call_model(self, state: State):\n",
        "        prompt = await prompt_template.ainvoke(state)\n",
        "\n",
        "        response = await self.model.ainvoke(prompt)\n",
        "\n",
        "        return {\"messages\": [response]}\n",
        "\n",
        "    async def ainvoke(self, state: dict, config: dict):\n",
        "        return await self.app.ainvoke(state, config)\n",
        "\n",
        "    async def astream(self, state: dict, config: dict):\n",
        "        async for chunk, metadata in self.app.astream(\n",
        "            state,\n",
        "            config,\n",
        "            stream_mode=\"messages\",\n",
        "        ):\n",
        "            if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
        "                yield chunk.content\n",
        "\n",
        "    def save_history(self):\n",
        "        save_history_to_file(self.history_file, self.messages)\n",
        "\n",
        "    def to_gradio_pair_msgs(self):\n",
        "        return to_gradio_pair_msgs(self.messages)"
      ],
      "metadata": {
        "id": "4WIT9A_NOjgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The UI - powered by Gradio"
      ],
      "metadata": {
        "id": "vEaZD13XtFmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LANGUAGE = \"English\"\n",
        "\n",
        "thread_id = \"user_001\"\n",
        "\n",
        "# config - thread_id is for when our chatbot having multiple conservations with many users\n",
        "# each thread_id process different conservation\n",
        "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "chatbot = Veo3Prompter(thread_id=thread_id)\n",
        "\n",
        "# returning full response method\n",
        "async def chat_fn(user_input, history):\n",
        "    msg = HumanMessage(user_input)\n",
        "    chatbot.messages.append(msg)\n",
        "\n",
        "    state = {\n",
        "        \"messages\": msg,\n",
        "        \"language\": LANGUAGE,\n",
        "    }\n",
        "\n",
        "    output = await chatbot.ainvoke(\n",
        "        state,\n",
        "        config,\n",
        "    )\n",
        "    chatbot.messages.append(content=output[-1])\n",
        "    chatbot.save_history()  # save after exchange\n",
        "    return output[\"messages\"][-1].content\n",
        "\n",
        "# streaming response method\n",
        "async def stream_fn(user_input, history):\n",
        "    msg = HumanMessage(user_input)\n",
        "    chatbot.messages.append(msg)\n",
        "\n",
        "    state = {\n",
        "        \"messages\": msg,\n",
        "        \"language\": LANGUAGE,\n",
        "    }\n",
        "\n",
        "    # yield chatbot.to_gradio_pair_msgs()[:]\n",
        "    bot_reply = \"\"\n",
        "    yield chatbot.to_gradio_pair_msgs()[:]\n",
        "\n",
        "    async for token in chatbot.astream(state, config):\n",
        "        if token:\n",
        "            bot_reply += token\n",
        "            yield chatbot.to_gradio_pair_msgs()[:-1] + [[user_input, bot_reply]]\n",
        "\n",
        "    chatbot.messages.append(AIMessage(content=bot_reply))\n",
        "    chatbot.save_history()  # save after exchange"
      ],
      "metadata": {
        "id": "esRHrz-VfrDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the interface (it will print a link you can click)\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## 🎬 Veo3 Prompt Chatbot\")\n",
        "    chatbot_ui = gr.Chatbot(label=\"Veo3 Assistant\", value=chatbot.to_gradio_pair_msgs(), elem_id=\"chatbox\", height=400)\n",
        "    user_input = gr.Textbox(show_label=False, placeholder=\"Describe your cinematic idea...\")\n",
        "\n",
        "    user_input.submit(stream_fn, inputs=[user_input, chatbot_ui], outputs=[chatbot_ui])\n",
        "    user_input.submit(lambda: \"\", None, user_input)  # clear textbox\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "738BTCyeavBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vo7snjn8nJRy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}